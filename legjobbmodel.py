import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold
from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, classification_report, confusion_matrix, f1_score, recall_score, roc_auc_score
from imblearn.over_sampling import SMOTE
import joblib
import json
def evaluate_models_pipeline(smote_strategy=0.91, test_size=0.2, file="model_results.json"):
    
    models_params_gridsearch = {
    "XGBoost": {
        "model": XGBClassifier(eval_metric='logloss', random_state=42),
        "params": {
            "n_estimators": [200, 500, 1000],
            "max_depth": [3, 5, 7],
            "learning_rate": [0.01, 0.1, 0.2],
            "subsample": [0.7, 0.8, 1.0]
        }
    },
    # "KNeighborsClassifier": {
    #     "model": KNeighborsClassifier(),
    #     "params": {
    #         "n_neighbors": [3, 5, 7]
    #     }
    # },
    "RandomForestClassifier": {
        "model": RandomForestClassifier(random_state=42),
        "params": {
            "n_estimators": [200, 500, 1000],
            "max_depth": [None, 5, 10, 15]
        }
    },
    # "SVC": {
    #     "model": SVC(probability=True),
    #     "params": {
    #         "C": [0.1, 1, 10],
    #         "gamma": ["scale", "auto"],
    #         "kernel": ["linear", "rbf"]
    #     }
    # }
}

    df = pd.read_csv("heart.csv")

    df.columns = df.columns.str.strip()

    X = df[['cp', 'trtbps', 'exng', 'slp', 'caa', 'thall']]
    y = df['output']

    smote = SMOTE(random_state=32, sampling_strategy=smote_strategy)
    X_resampled, y_resampled = smote.fit_resample(X, y)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_resampled, y_resampled, test_size=test_size, random_state=32, stratify=y_resampled
    )
    
    # Print the new data points added by SMOTE
    print("New datapoints generated by SMOTE:")
    new_data_points = pd.DataFrame(X_resampled[len(X):], columns=X.columns)
    new_data_points["output"] = y_resampled[len(X):]
    print(new_data_points)

    # Print original and resampled dataset sizes
    print("\nOriginal dataset sizes:")
    print(f"X: {len(X)}, y: {len(y)}")

    print("y value_counts:")
    print(y.value_counts())

    print("\nResampled dataset sizes:")
    print(f"X_resampled: {len(X_resampled)}, y_resampled: {len(y_resampled)}")
    print("y_resampled value_counts:")
    print(y_resampled.value_counts())

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    rf = RandomForestClassifier(max_depth=5, n_estimators=100)
    # return rf.fit(X_train_scaled, y_train)

    stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    best_model = None
    best_model_name = None
    best_params = None
    best_score = 0
    best_accuracy = 0
    best_conf_mx = None
    best_class_report = None
    best_roc_auc = 0

    for model_name, mp in models_params_gridsearch.items():
        clf = GridSearchCV(
            mp['model'],
            mp['params'],
            cv=stratified_kfold,
            scoring='roc_auc',
            return_train_score=False,
            n_jobs=-1
        )
        
        clf.fit(X_train_scaled, y_train)
        model = clf.best_estimator_
        model.fit(X_train_scaled, y_train)

        # Step 6: Evaluation
        y_pred = model.predict(X_test_scaled)
        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]

        conf_mx = confusion_matrix(y_test, y_pred, labels=[0, 1])
        class_report = classification_report(y_test, y_pred, target_names=['lower chance of heart disease', 'higher chance of heart disease'], digits=5)

        roc_auc = roc_auc_score(y_test, y_pred_proba)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        accuracy = accuracy_score(y_test, y_pred)

        avg_score = (roc_auc + recall) / 2

        if avg_score > best_score:

            best_model = model

            best_score = avg_score
            best_model_name = model_name
            best_params = clf.best_params_
            best_accuracy = accuracy
            best_f1score = f1
            best_conf_mx = conf_mx.tolist()
            best_class_report = class_report
            best_roc_auc = roc_auc

    best_result = {
        "smote": smote_strategy,
        "test_size": test_size,
        "model_name": best_model_name,
        "best_params": best_params,
        "accuracy": best_accuracy,
        "avg_score": best_score,
        "f1_score":best_f1score,
        # "confusion_matrix": best_conf_mx,
        "roc_auc": best_roc_auc,
        "classification_report": best_class_report
    }

    best_conf_mx = np.array(best_conf_mx)

    print("\n===== Best Model Results =====\n")
    print(f"Model Name: {best_model_name}")
    print(f"SMOTE Strategy: {smote_strategy}")
    print(f"Test Size: {test_size}")
    print(f"Best Parameters: {best_params}")
    print(f"Accuracy: {best_accuracy:.4f}")
    print(f"Average Score: {best_score:.4f}")
    print(f"F1 Score: {best_f1score:.4f}")
    print(f"ROC AUC: {best_roc_auc:.4f}\n")

    print("Classification Report:\n")
    print(best_class_report)

    print("\nConfusion Matrix:")
    print(best_conf_mx)

    disp = ConfusionMatrixDisplay(confusion_matrix=best_conf_mx, display_labels=["more chance", "less chance"])
    disp.plot(cmap='Blues')
    disp.ax_.set_title("Confusion Matrix - XGBoost")  # Set title directly on the Axes object
    plt.savefig("XGBoost_conf_mx.png")
    plt.show()


    # with open(file, "w") as f:
    #     json.dump(best_result, f, indent=4)
    
    # joblib.dump(model, "XGBoost_model_proba.joblib")

if __name__ == '__main__':
    for i in range(3):
        evaluate_models_pipeline()